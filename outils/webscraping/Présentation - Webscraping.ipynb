{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Introduction au Webscraping</h2>\n",
    "<h3>Principes fondamentaux et mise en pratique</h3>\n",
    "</center>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Objectifs de la présentation\n",
    "\n",
    "* Introduire les concepts et éléments techniques fondamentaux du _webscraping_.\n",
    "* Présenter les étapes-clefs d'une tâche _webscraping_ et leur articulation.\n",
    "* Illustrer la mise en place d'un webscraping avec différents outils, en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notions fondamentales du _webscraping_\n",
    "\n",
    "* Le _webscraping_ est une activité de collecte (et généralement d'extraction) automatisée de données en ligne.\n",
    "  * Il s'agit d'accéder de manière automatisée à des pages web (et autres ressources en ligne) en vue d'en extraire\n",
    "    certains contenus.\n",
    "    \n",
    "  * Une première étape est donc d'accéder à la page \"contenant\" l'information que l'on cherche à extraire.\n",
    "  \n",
    "  * La seconde étape consiste alors à extraire et structurer l'information contenue dans cette page.\n",
    "\n",
    "\n",
    "* Il est donc nécessaire de savoir sur quelle(s) page(s) récupérer l'information.\n",
    "  * Dans certains cas, on sait exactement quelle(s) page(s) on souhaite consulter : on peut alors simplement en\n",
    "    établir la liste et scraper celles-ci. _E.g. : scraper une liste de pages d'accueils de journaux pour extraire\n",
    "    le titre des articles en \"Une\"._\n",
    "   \n",
    "  * Dans d'autres cas, on peut établir des règles de navigation automatisée, afin de \"découvrir\" les pages à scraper\n",
    "    à partir d'un ou plusieurs points d'entrée. _E.g. : scraper la page d'accueil du journal Le Monde, trouver tous\n",
    "    les liens vers des articles puis scraper ceux-ci afin d'en récupérer le titre, l'auteur et (si possible) le texte._\n",
    "    \n",
    "  * La navigation automatisée le long des liens extraits à partir d'un point d'entrée est appellée _web crawling_.\n",
    "\n",
    "\n",
    "* Il faut ensuite définir des règles d'extraction de l'information.\n",
    "  * Dans certains cas, on connaît la structure des pages que l'on scrape : on peut alors définir assez finement des\n",
    "    règles de sélection de l'information, afin de sélectionner exactement ce que l'on veut, de manière déjà structurée.\n",
    "    _E.g. : tous les articles du site Le Monde suivent un modèle similaire, il est donc possible d'étudier celui-ci\n",
    "    pour définir des règles d'extraction précises, valides pour tous les articles ciblés._\n",
    "  \n",
    "  * Dans d'autres cas, l'hétérogénéité des pages que l'on scrape rend très couteuse la définition de telles règles. Des\n",
    "    règles plus générales, qui risquent cependant d'être moins précises, peuvent alors être utilisées. _E.g. : si l'on\n",
    "    cible tous les articles référencés sur Google News, la diversité des médias rend difficile le scraping \"fin\" des\n",
    "    pages. On va alors définir des règles assez générales pour identifier le texte de l'article, quitte à définir des\n",
    "    règles plus fines pour quelques sites jugés majeurs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Éléments techniques fondamentaux "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 Requêter une page web\n",
    "\n",
    "* URL d'une ressource\n",
    "  * Uniform Resource Locator, ou URL, est une norme d'adressage des ressources sur internet.\n",
    "  \n",
    "  * Une URL spécifie (_a minima_) un protocole d'accès à la ressource et la localisation de celle-ci.\n",
    "    Cette localisation comprend notamment un nom de domaine (ou une adresse IP). Il est également possible\n",
    "    de spécifier un certain nombre d'options.\n",
    "    \n",
    "  * Exemple : `https://duckduckgo.com/?q=webscraping`. Il s'agit de requêter DuckDuckGo à l'aide du protocole HTTPS\n",
    "    (cf. infra), et plus spécifiquement d'effectuer _via_ le service fourni par cet excellent moteur de recherche\n",
    "    la recherche du terme \"webscraping\".\n",
    "  \n",
    "  * Afin de collecter une page web, on va donc effectuer une requête vers son URL, en spécifiant éventuellement\n",
    "    certains paramètres.\n",
    "\n",
    "\n",
    "* Requêtes HTTP\n",
    "  * Hyper Text Transfer Protocol, ou HTTP, est un protocole de communication entre un client (e.g. un navigateur web)\n",
    "    et un serveur (e.g. le serveur hébergeant un site web) développé pour le web.\n",
    "    * Une variante communément rencontrée est HTTPS ; le S abrège \"Secured\" et désigne le recours conjoint à d'autres\n",
    "      protocoles (SSL ou TLS) pour sécuriser l'échange (authentification du serveur, chiffrement des données...).\n",
    "    * Il existe d'autres protocoles permettant d'échanger des données sur internet ; ils ne relèvent cependant pas du\n",
    "      _webscraping_ à proprement parler, aussi ne les mentionnerons-nous pas.\n",
    "    \n",
    "  * HTTP permet d'effectuer plusieurs types de requête, dont la principale est GET, qui permet de demander une ressource.\n",
    "    D'autres requêtes, dont POST (ou PUT), peuvent être utiles dans certains cas lorsque l'on scrape un site (nous y\n",
    "    reviendrons).\n",
    "\n",
    "  * À l'issue d'une requête HTTP, un code d'état est assigné à son résultat. Celui-ci permet de caractériser le\n",
    "    succès de la requête, ou de spécifier la nature d'un échec le cas échéant. Il est donc très utile pour ajuster\n",
    "    le comportement d'un programme de _webscraping_ suivant le type d'erreur qu'il peut rencontrer. Voici quelques\n",
    "    codes qu'il est utile de connaître :\n",
    "    * Le code 200 indique le succès de la requête. (_on peut donc exploiter la ressource_)\n",
    "    * Les codes 301 et 302 indiquent une redirection (permanente ou temporaire) vers une autre URL à partir\n",
    "      de celle requêtée. (_on va donc vérifier que la ressource correspond à celle recherchée_)\n",
    "    * Le code 403 indique que l'accès à la ressource demandée a été refusé. (_on va donc abandonner la ressource, ou\n",
    "      réessayer plus tard s'il s'agit d'une interdiction liée à une restriction du nombre de requêtes authorisées_)\n",
    "    * Le code 404 indique que la ressource demandée n'a pas été trouvée (_il faut donc abandonner la ressource, ou\n",
    "      rechercher sa nouvelle adresse si elle existe_).\n",
    "    * Le code 443 indique un blocage de la requête par le Proxy (_il faut donc réviser la configuration de l'accès à\n",
    "      celui-ci, potentiellement (dans le cadre institutionnel) en concertation avec les équipes informatiques locales_).\n",
    "    * Les codes 500 et 503 indiquent des erreurs au niveau du serveur (_on peut donc tenter de requêter à nouveau, tout\n",
    "      de suite ou plus tard_).\n",
    "\n",
    "\n",
    "\n",
    "* Requêter une page web en Python.\n",
    "  * `Requests` est une librairie tierce pour Python, construite au-dessus de la librairie standard `urllib3`.\n",
    "    Elle est documentée <a href=\"http://docs.python-requests.org/en/master/\">ici</a>.\n",
    "  * Conçue pour être facile d'accès, elle est idéale pour cette présentation ; chacun est ensuite libre de\n",
    "    lui préférer la librairie standard, ou une autre librairie tierce. Dans tous les cas, la syntaxe change\n",
    "    (un peu), mais le principe reste le même, et les mécanismes sous-jacents sont strictement identiques.\n",
    "  * Pour les adeptes de R, `RCurl` est plus ou moins l'équivalent de `urllib3`, tandis que `httr` semble un\n",
    "    peu plus accesible et que `rvest` est plus spécifiquement adapté au _webscraping_. Pour une documentation\n",
    "    des packages existants pour faire des requêtes (et extraire de l'information des pages web), consulter\n",
    "    <a href=\"https://cran.r-project.org/web/views/WebTechnologies.html\">cette page</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requête GET sur l'url correspondant à la page d'accueil de l'Insee.\n",
    "response = requests.get('https://insee.fr/fr/accueil')\n",
    "\n",
    "print('Url: %s' % response.url)\n",
    "print('Statut: %s' % response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question : Génial, mais alors qu'est-ce qu'on a récupéré ? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Réponse : le code source de la page d'accueil de l'INSEE. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Le HTML, code source des pages web.\n",
    "\n",
    "#### Qu'est-ce que le HTML ?\n",
    "\n",
    "* Hyper Text Markup Language (HTML) est le langage utilisé pour écrire des pages web. Il s'agit d'un langage de balisage \n",
    "  (comme LaTeX et XML, notamment) : des balises (de forme `<balise>`) structurent le contenu de la page et dessinent une \n",
    "  arborescence de contenus.\n",
    "    \n",
    "\n",
    "* La page visualisée dans un navigateur web est générée localement à partir du code HTML.\n",
    "  * La connexion a un site entraîne la requête et le téléchargement du HTML de la page (et de ressources associées).\n",
    "  * Le rôle du navigateur web est alors de \"rendre\" le HTML, c'est-à-dire de générer le contenu que l'on visualise\n",
    "    et avec lequel on est amené à intéragir.\n",
    "  * Note : les interactions proviennent de la définition d'événements pouvant (ou non) entraîner l'exécution de\n",
    "    requêtes supplémentaires (asynchrones) ou la modification de propriétés associées à des balises déjà chargées.\n",
    "  * En particulier, il est possible d'intégrer nativement du code en JavaScript (langage de programmation interprété)\n",
    "    dans la page, dont l'interprétation revient au navigateur. Pour d'autres technologies (et dans certains cas pour\n",
    "    du JavaScript), les opérations ont lieu du côté du site.\n",
    "  * Afin de spécifier les paramètres de style des éléments (taille, couleur, etc.), il est également d'usage de\n",
    "    recourir au langage CSS (Cascade Style Sheet), souvent chargé depuis un ou plusieurs fichiers `.css`. Ce n'est\n",
    "    cependant (en général) pas cela qui nous intéresse dans le cadre du scraping, et nous laisserons donc cet aspect\n",
    "    de côté.\n",
    "\n",
    "\n",
    "* Pour bien le comprendre, on peut recourir à un outil intégré à certains navigateurs (en particulier,\n",
    "  Mozilla Firefox) : l'inspecteur d'éléments. Celui-ci permet de consulter le html de la page actuellement\n",
    "  rendue, et de le modifier (localement !), altérant ainsi la page rendue dans le navigateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Éléments fondamentaux de structure d'une page HTML\n",
    "\n",
    "* La structuration du HTML se fait autour de balises.\n",
    "  * La plupart des balises fonctionnent par paire : une balise ouvrante `<balise>` et une balise fermante `</balise>`.\n",
    "  \n",
    "  * Quelques balises sont auto-fermantes ; il est alors de bon aloi de les écrirer `<balise/>`.\n",
    "    La plus typique est la balise de retour à la ligne : `<br/>` (qui connaît des variantes suivant les conventions\n",
    "    de la personne (ou du programme) écrivant le html.\n",
    "    \n",
    "  * Une balise possède un type, qui est le premier mot de la balise ouvrante (et le seul de la balise fermante).\n",
    "    Elle peut aussi être dotée d'attributs, écrits dans la balise ouvrante, au format `attribut=\"valeur\"`.\n",
    "    Quelques exemples : \n",
    "    * `<div id=\"toto\">` est une `div` (division) dont l'attribut `id` est `toto`.\n",
    "    * `<p style=\"font-size: 18px;\">` est un `p` (paragraphe) dont le texte va être rendu dans une police de 18 pixels.\n",
    "    * `<a href=\"https://insee.fr/\">` est un `a` (lien) pointant vers https://insee.fr/.\n",
    "\n",
    "  * Il existe un grand nombre de balises et d'attributs \"standards\", qui pour certains impliquent par soi un\n",
    "    comportement spécifique (e.g. `<ul>` définit une liste sans ordre (_unordered list_), si bien que chaque\n",
    "    balise `<li>` va générer une puce devant son contenu). Il est également possible de définir arbitrairement\n",
    "    des balises ou attributs, en respectant ou non les conventions préconisées par le\n",
    "    <a href=\"https://www.w3schools.com/default.asp\">W3C</a> (World Wide Web Consortium).\n",
    "\n",
    "* Trois balises fournissent la structure essentielle de toute page html :\n",
    "  * `<html>` : balise principale, contenant l'intégralité de la page.\n",
    "  * `<header>` : espace où sont notamment renseignées des méta-informations, et où sont souvent chargées ou\n",
    "    définies des ressources \"non affichées\" (tels des scripts ou règles de style graphique).\n",
    "  * `<body>` : espace où se trouve le contenu visible de la page ; il est également fréquent d'y rencontrer\n",
    "    des scripts définissant des actions spécifiques.\n",
    "  \n",
    "\n",
    "* Quelques types de balises essentiels :\n",
    "  * `<h1>` (et `<h2>`, etc.) : balises définissant des styles de titre.\n",
    "  * `<div>` : \"division\" ; cette balise est l'une des plus usuelles et diverses.\n",
    "  * `<p>` : \"paragraphe\", supposé encadrer du texte.\n",
    "  * `<a>` : \"lien\" (_anchor_), pouvant pointer sur une section de la page ou vers une autre adresse.\n",
    "  * `<img/>` : balise auto-fermante permettant de charger un image dans la page.\n",
    "  * `<script>` : script JavaScript, pouvant parfois contenir des informations d'intérêt (on y reviendra).\n",
    "  * `<meta/>` : balise auto-fermante indiquant une méta-information, présente dans le `<header>`\n",
    "\n",
    "\n",
    "* Quelques attributs de balises essentiels :\n",
    "  * `id` : identifiant (nommé) ne pouvant être partagé avec une autre balise.\n",
    "  * `class` : identifiant (nommé) pouvant être commun à plusieurs balises.\n",
    "  * `href` : adresse vers laquelle pointe la balise (typiquement: `<a href=\"url\">texte du lien</a>`)\n",
    "  * `src` : source d'une ressource chargée par la balise (e.g. `<img src=\"image.png\" />`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Écrivons une page html basique, qui dit \"Hello World\".\n",
    "html = \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Hello World</title>\n",
    "        <meta author=\"Paul Andrey\"/>\n",
    "    </head>\n",
    "    <body>\n",
    "        <center>Hello World!</center>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "with open('hello.html', mode='w', encoding='utf-8') as html_file:\n",
    "    html_file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Par la suite, nous rendrons le HTML directement dans le notebook, comme suit :\n",
    "from IPython.core.display import HTML\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Nous savons donc requêter le HTML source d'une page dont l'URL est connue, et savons que ce HTML est structuré.<br/>\n",
    "Mais en pratique, comment extraire l'information qu'il contient ?**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br/>\n",
    "## 2. Extraire de l'information d'une page HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1 Beautiful Soup\n",
    "\n",
    "* Beautiful Soup (`bs4`) est une librairie tierce pour Python permettant de parser (i.e. analyser syntaxiquement) divers langages de balisage, et notamment le HTML. Elle s'appuie en partie sur la librairie standard `html`, et fournit une interface plutôt pratique.\n",
    "  * La documentation de Beautiful Soup est consultable <a href=\"https://www.crummy.com/software/BeautifulSoup/\">\n",
    "    à cette adresse</a>.\n",
    "  * D'autres parseurs existent, et aucun n'est parfait puisque le HTML rencontré \"dans la vraie vie\" est rarement exempt\n",
    "    de \"fautes\" de syntaxe, auxquelles les parseurs s'ajustent diversement selon les cas.\n",
    "  * Citons notamment la librairie `lxml`, qui permet notamment de travailler sur du XML ou du HTML ; nous ne\n",
    "    l'utiliserons pas ici, cependant elle est assez aisée à prendre en main et présente une logique assez similaire\n",
    "    avec celle de BeautifulSoup.\n",
    "\n",
    "\n",
    "* BeautifulSoup est simple d'utilisation, et flexible :\n",
    "  * On instancie un objet de classe `bs4.BeautifulSoup` à partir d'une donnée textuelle ou d'un fichier ouvert\n",
    "    avec `open`, en spécifiant éventuellement le parseur à utiliser (`html.parser` pour html), quoi que celui-ci\n",
    "    puisse normalement faire l'objet d'une détection automatique.\n",
    "  * Il est alors possible de chercher une ou plusieurs balises au sein du HTML à partir de critères comme le type\n",
    "    de balise, la valeur d'un ou plusieurs de ses attributs ou encore sa position relative à d'autres balises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parsons le html de la page d'accueil de l'Insee.\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cherchons la première balise <div> de la page.\n",
    "first_div = soup.find('div')\n",
    "first_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On peut accéder aux attributs de la balise, sous forme de dictionaire.\n",
    "first_div.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On peut également accéder à son contenu, sous forme de liste de sous-balises.\n",
    "first_div.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Une autre possibilité est d'itérer sur les sous-balises. Ici, on affiche leur type.\n",
    "for tag in first_div.children:\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On peut aussi accéder au texte apparaissant à l'intérieur de la balise (et de ses sous-balises).\n",
    "first_div.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On peut maintenant chercher la seconde <div> de la page:\n",
    "second_div = first_div.find_next_sibling()\n",
    "second_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ou bien la première balise suivant la première <div> - qui sera ici la première sous-balise!\n",
    "first_div.find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cherchons maintenant toutes les balises de lien (balises <a>) dans la page.\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On voit que les liens internes appartiennent à la classe \"lien\" ; cherchons ceux-ci :\n",
    "soup.find_all('a', {'class': 'lien'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    \"\"\"Retire les sauts de ligne, puces et indentations d'une chaîne de caractères donnée.\"\"\"\n",
    "    return string.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace('\\xa0', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extrayons proprement les liens internes au site de l'Insee trouvés sur la page d'accueil.\n",
    "liens_insee = [\n",
    "    (clean_text(a.text), 'https://insee.fr' + clean_text(a['href']))\n",
    "    for a in soup.find_all('a', {'class': 'lien'})\n",
    "]\n",
    "liens_insee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quels sont les liens dont le descriptif mentionne le mot \"européen\" ?\n",
    "[\"%s : %s\" % a for a in liens_insee if 'européen' in a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mince, il y a un doublon. Dédupliquons les liens listés, et recommençons.\n",
    "liens_insee = list(set(liens_insee))\n",
    "[\"%s : %s\" % a for a in liens_insee if 'européen' in a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Si maintenant on veut scraper ces pages, rien de plus simple :\n",
    "pages_europe = [\n",
    "    requests.get(url)\n",
    "    for description, url in liens_insee\n",
    "    if 'européen' in description\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages_europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text(response):\n",
    "    \"\"\"Fonction rapide d'extraction du texte dans une page requêtée avec Requests.\"\"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = '. '.join(\n",
    "        clean_text(paragraph.text)\n",
    "        for paragraph in soup.find_all('p')\n",
    "    )\n",
    "    return text.replace(' .', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ce n'est pas parfait, mais c'est un début.\n",
    "list(map(extract_text, pages_europe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé des éléments fondamentaux de BeautifulSoup :\n",
    "\n",
    "* Importer la classe BeautifulSoup : `from bs4 import BeautifulSoup`\n",
    "* Parser un texte html et l'assigner à une variable `soup` : `soup = BeautifulSoup(html_string, 'html.parser')`\n",
    "* Parser le html obtenu via une requête `Requests.get` réussie : `soup = BeautifulSoup(response.text, 'html.parser')`\n",
    "\n",
    "\n",
    "* Recherche d'une ou plusieurs balises dans le document parsé (ici noté `soup`):\n",
    "  * `soup.find()` : retourne la première balise répondant aux critères spécifiés.\n",
    "  * `soup.find_all()` : retourne la liste de toutes les balises répondant aux critères.\n",
    "  \n",
    "  \n",
    "* Les critères de recherchent peuvent porter sur :\n",
    "  * Le type de balise (premier argument des fonctions de recherche).\n",
    "    * un seul type : `soup.find('div')`\n",
    "    * ou plusieurs : `soup.find(['h1', 'h2'])`\n",
    "  * Un ou plusieurs attributs de la ou des balises recherchées.\n",
    "    * les attributs comme `id` ou `href` peuvent être passés comme arguments : `soup.find(id='toto')`\n",
    "    * certains ne passent pas ainsi (notamment `class`, qui est un mot-clef réservé de Python) ;\n",
    "      on passe alors un dictionaire : `soup.find_all({'class': 'toto'})`.\n",
    "  * Le texte contenu dans la ou les balises recherchées.\n",
    "    * `soup.find(text=\"Fil d'actualités\")`\n",
    "    * Pour le texte (et parfois pour les attributs), on aura tendance à vouloir des critères plus\n",
    "      souples qu'une adéquation stricte : on utilisera alors les expressions régulières, comme\n",
    "      nous le montrerons par la suite.\n",
    "\n",
    "\n",
    "* Accès aux éléments associés à une balise (ici notée `tag`):\n",
    "  * `tag.name` : type de balise.\n",
    "  * `tag.attrs` : attributs de la balise, sous forme de dictionaire.\n",
    "  * `tag.get()` : accès à un attribut, avec valeur par défaut spécifiable s'il n'existe pas (`̀tag.get('id', '')`).\n",
    "  * `tag.text` : accès aux textes contenus dans la balise et ses sous-balises, concaténés.\n",
    "  * `tag.get_text(separator=' ')` : similaire au précédent, en spécifiant un séparateur entre textes concaténés.\n",
    "  * `tag.contents` : liste des sous-balises.\n",
    "\n",
    "\n",
    "* Recherche d'une ou plusieurs balises relativement à une autre balise (ici notée `tag`):\n",
    "  * ̀`tag.find_next()` : retourne la prochaine balise répondant aux critères spécifiés.\n",
    "  * `tag.find_next_sibling()` : retourne la prochaine balise du même type que `tag` et répondant aux critères spécifiés.\n",
    "  * `tag.find_all_next()` : retourne toutes les balises postérieures à la présente et répondant aux critères spécifiés.\n",
    "  * `tag.find_next_siblings()` : retourne toutes les balises postérieures de même type et répondant aux critères\n",
    "     spécifiés.\n",
    "      * _Nota Bene_ : en remplaçant `next` par `previous` pour chacune des méthodes précédentes, on accès aux balises\n",
    "        antérieures à `tag` selon la même logique.\n",
    "  * `tag.find_parent()` : retourne la plus proche balise englobante répondant aux critères spécifiés.\n",
    "  * `tag.find_parents()` : liste les balises englobant la présente et répondant aux critères spécifiés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Les expressions régulières\n",
    "\n",
    "* Les expressions régulières (aussi appelées expressions rationnelles et souvent abrégées _regex_ ou _regexp_)\n",
    "  sont une chaîne de caractères définissant, selon une syntaxe formelle précise, un ensemble de chaînes de\n",
    "  caractères possibles. Il s'agit donc d'un outil souple et efficace pour (notamment) faire des recherches\n",
    "  dans un texte ou le transformer.\n",
    "\n",
    "\n",
    "* On ne fera pas ici un cours sur les expressions regulières (ce n'est pas le sujet) ; l'excellent site\n",
    "  <a href=\"http://www.regexr.com/\">regexr</a> peut être utilisé pour s'y familiariser, voire pour effectuer\n",
    "  des tests même une fois la syntaxe maîtrisée.\n",
    "\n",
    "\n",
    "* Les expressions régulières sont implémentées dans Python comme dans R, quoi que ce dernier langage ajoute des\n",
    "  spécificités demandant de bien lire la documentation (`?regex`), notamment concernant les différents modes de\n",
    "  compatibilité des expressions.\n",
    "  \n",
    "  \n",
    "* La librairie standard `re` permet d'utiliser les expressions régulières en Python :\n",
    "  * `re.search(pattern, text)` : rechercher la première occurence d'un pattern dans un texte.\n",
    "  * `re.findall(pattern, text)` : rechercher toutes les occurences d'une pattern dans un texte.\n",
    "  * `re.compile(pattern)` : pré-compiler un pattern (utile dans certains contextes, comme on le verra).\n",
    "  * `re.sub(pattern, remplacement, text)` : rechercher et remplacer un pattern par une chaîne de caractères\n",
    "     dans un texte.\n",
    "  * `re.split(pattern, text)` : découper un texte (similairement à `str.split()`) le long d'un pattern.\n",
    " \n",
    " \n",
    "* Il peut être pratique d'utiliser des expressions régulières sur du html. Cependant, c'est très rarement une bonne\n",
    "  idée de parser du html avec des expressions régulières, car à la moindre faute de syntaxe (ou subtilité syntaxique\n",
    "  non anticipée), on va se planter. Nous allons cependant voir que les expressions régulières peuvent être utilisées\n",
    "  de manière complémentaire avec BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identifions toutes les balises <a> dans la page d'accueil de l'Insee. Cela fonctionne, même si c'est mal.\n",
    "import re\n",
    "\n",
    "balises_a = re.findall('<a .+?</a>', response.text)\n",
    "balises_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similairement à ce que l'on a déjà fait, identifions les liens internes au site de l'Insee, avec des regex.\n",
    "liens_insee_regex = [\n",
    "    (\n",
    "        clean_text(re.search('(?<=>).+?(?=<)', a).group()),\n",
    "        'https://insee.fr' + clean_text(re.search('(?<=href=\").+?(?=\")', a).group())\n",
    "    )\n",
    "    for a in balises_a if 'class=\"lien\"' in a\n",
    "]\n",
    "# Sans oublier de dédoublonner.\n",
    "liens_insee_regex = list(set(liens_insee_regex))\n",
    "# Est-ce bien le même résultat ? Non ; on a manqué des liens, semble-t-il.\n",
    "liens_insee == liens_insee_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alors, comment _bien_ utiliser les expressions régulières ?\n",
    "\n",
    "* On peut _parfois_ chercher un élément directement dans le html avec les expressions régulières, si l'on est\n",
    "  vraiment sûr que ce sera le bon et que BeautifulSoup ne le permet pas (ou pas aussi facilement). Mais cela\n",
    "  reste une pratique risquée et peu conseillée.\n",
    "\n",
    "\n",
    "* On peut sutout utiliser les expressions régulières **conjointement à BeautifulSoup** :\n",
    "  * Pour spécifier des critères de recherche au niveau de BeautifulSoup : une expression régulière\n",
    "    compilée avec `re.compile` peut être passée en critère de recherche, pour la valeur d'un attribut\n",
    "    ou du texte contenu dans une balise.\n",
    "  * Pour nettoyer et/ou filtrer un contenu déjà sélectionné à l'aide de BeautifulSoup : il est parfois\n",
    "    plus efficace de mettre un critère de recherche peu restrictif, et de filtrer les résultats à l'aide\n",
    "    d'une expression régulière (compiler une expression et rechercher à son aide peut s'avérer coûteux).\n",
    "    Les expressions regulières sont aussi un excellent outil pour nettoyer du texte, ou en extraire une\n",
    "    sous-partie !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cherchons les liens pointant sur un autre site que celui de l'Insee.\n",
    "# Ces liens commencent par 'http' (le protocole) car ils sont absolus, pas relatifs.\n",
    "# NB : on peut avoir un lien \"interne\" absolu, le critère retenu ici n'est pas le plus efficace !\n",
    "liens_externes = [\n",
    "    clean_text(a['href']) for a in soup.find_all('a', href=re.compile('^http'))\n",
    "]\n",
    "liens_externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plutôt que de compiler l'expression régulière, on peut faire un filtre post-sélection.\n",
    "# C'est bien aussi, et surtout plus rapide (dans le cas présent).\n",
    "liens_externes = [\n",
    "    clean_text(a['href']) for a in soup.find_all('a')\n",
    "    if re.search('^http', a.get('href', '')) is not None\n",
    "]\n",
    "liens_externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mais si l'on réfléchit, dans cet exemple précis, les regex sont en fait inutiles...\n",
    "liens_externes = [\n",
    "        clean_text(a['href']) for a in soup.find_all('a')\n",
    "        if a.get('href', '').startswith('http')\n",
    "    ]\n",
    "liens_externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testons les temps d'exécution des trois solutions précédentes.\n",
    "from timeit import timeit\n",
    "\n",
    "def solution_regex_compil():\n",
    "    liens_externes = [\n",
    "    clean_text(a['href']) for a in soup.find_all('a', href=re.compile('^http'))\n",
    "]\n",
    "def solution_regex_filtre():\n",
    "    liens_externes = [\n",
    "    clean_text(a['href']) for a in soup.find_all('a')\n",
    "    if re.search('^http', a.get('href', '')) is not None\n",
    "]\n",
    "def solution_sans_regex():\n",
    "    liens_externes = [\n",
    "        clean_text(a['href']) for a in soup.find_all('a')\n",
    "        if a.get('href', '').startswith('http')\n",
    "    ]\n",
    "    \n",
    "for function in [solution_regex_compil, solution_regex_filtre, solution_sans_regex]:\n",
    "    temps = timeit(function, number=1000)\n",
    "    print('%s : %s' % (function.__name__, temps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Selon les tâches, la combinaison d'outils est donc à adapter !</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Identifier efficacement les cibles\n",
    "\n",
    "* Le degré souhaitable d'identification de la structure de la page va différer selon la tâche que l'on résout.\n",
    "  * Un premier élément est bien sûr la nature de l'information reccueillie : chercher des liens ou images est\n",
    "    globalement plus simple que chercher du texte, car les balises les abritant sont très spécifiques. Il peut\n",
    "    alors rester à définir des filtres adéquats. Les points suivants portent surtout sur des cas où l'on cherche\n",
    "    à récolter une information plus spécifique (texte ou champs spécifiques, par exemple).\n",
    "\n",
    "  * Dans le cas d'un _scraping_ portant sur une diversité de sources (_scraping_ dit \"générique\"), on va surtout\n",
    "    réfléchir au type de balises que l'on vise, aux filtres éventuels que l'on peut appliquer sur les résultats\n",
    "    (présence de certains mots-clefs dans un texte par exemple) et à un certain mode de structuration des données\n",
    "    permettant de rendre compte de ce qu'elles sont à l'origine.\n",
    "    * _Par exemple, si l'on récupère des articles de presse sur de nombreux sites différents, on peut chercher\n",
    "      à identifier les sections correspondant aux menus et aux commentaires (pour les éliminer), pour se focaliser\n",
    "      sur ce qui semble être le contenu principal (selon le nom des balises l'encadrant, la présence de titres,\n",
    "      la longueur du texte...)._\n",
    "  * Dans le cas d'un _scraping_ portant sur une source précise (ou plusieurs sources distinctes faisant l'objet\n",
    "    de traitements d'extraction différenciés), on va chercher à acquérir une information plus fine sur la structuration\n",
    "    des pages ciblées. Pour cela, on fait une espèce de _reverse engineering_ à partir de l'observation de pages\n",
    "    de test, pour définir des règles précises d'extraction de l'information.\n",
    "\n",
    "\n",
    "* Afin d'étudier la structure des pages sur un site, plusieurs outils peuvent être très pratiques :\n",
    "    * Un outil déjà mentionné est l'inspecteur d'éléments de Firefox (dont des équivalents existent\n",
    "      pour certains autres navigateurs, comme Chromium). Il permet de visualiser le code source de\n",
    "      la page actuellement rendue, de l'altérer pour constater des changements induits \"en direct\",\n",
    "      et surtout de trouver la portion de code associée à un élément visuel d'un simple clic droit\n",
    "      (\"inspecter l'élément\").\n",
    "    \n",
    "    * Plus généralement, regarder le code source d'une page (ctrl + u dans un navigateur web) est toujours\n",
    "      une bonne idée. Il s'agit d'en comprendre l'organisation globale, de comparer la page rendue à son code\n",
    "      source pour comprendre quelle section fait quoi, et d'identifier où se trouvent les éléments que l'on\n",
    "      souhaite extraire, à grands renforts de ctrl + f (rechercher) si besoin.\n",
    "      \n",
    "    * Enfin, il ne pas oublier de regarder s'il n'y as pas \"plus\" ou \"mieux\" que ce que l'on cherche initialement,\n",
    "      à d'autres endroits du code source que ceux rendus visuellement. En particulier, des scripts sont susceptibles\n",
    "      de comporter des variables structurant au format JSON une information par la suite répartie dans des structures\n",
    "      visuelles lors du rendu de la page. Ces variables, écrites en clair, sont généralement faciles à récupérer par\n",
    "      une combinaison BeautifulSoup / RegExp, et peuvent être parsée à l'aide de la librairie standard `json`, qui\n",
    "      va transformer la chaîne de caractères obtenue en une structure à base de dictionaires et de listes. Et si\n",
    "      `json` ne fonctionne pas, on peut aussi essayer `yaml` (librairie tierce `pyyaml`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Démonstration : scrapons des offres d'emploi.\n",
    "\n",
    "* On part de l'url des pages de recherche d'offres d'emploi.\n",
    "* On trouve le paramètre permettant d'itérer sur les pages de recherche.\n",
    "* On identifie une caractéristique des liens d'offres d'emploi, on en déduit une fonction pour identifier ces liens.\n",
    "* On étudie maintenant une page d'offre d'emploi. On trouve ce que l'on a envie d'extraire dans le code source.\n",
    "* On galère un peu avec json / yaml, on finit par trouver, on écrit la fonction de parsing magique.\n",
    "* On réfléchit cinq minutes au format de stockage des données. En général, csv c'est très bien.\n",
    "* Et hop, il n'y a plus qu'à organiser les \"bonnes\" itérations pour scraper !\n",
    "* (Si on veut faire propre, on formalise le tout, on gère les erreurs, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ici, faire de la magie en suivant la to-do list précédente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compléments et ouvertures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Quelques éléments de législation et de bonnes pratiques\n",
    "\n",
    "* La législation sur la _scraping_ est globalement floue. Cette pratique est globalement tolérée, mais un site\n",
    "  peut s'y opposer (conditions d'utilisation, dont le champ d'application est cependant incertain), et en tout\n",
    "  cas (tenter de) la réguler en spécifiant des règles d'accès _via_ le fichier `robots.txt`.\n",
    "  \n",
    "  \n",
    "* (Presque) tous les sites sont dotés d'un fichier `robots.txt`, à l'adresse `http(s)://<nom_de_domaine>/robots.txt`.\n",
    "  Celui-ci spécifie des règles d'interdiction de pages ou sections de l'arborescence du site, pouvant être différenciés\n",
    "  par agent d'utilisation. Il peut aussi spécifier des délais (en secondes) à respecter entre deux requêtes.\n",
    "  Globalement, c'est bien de respecter ces règles, même si on peut avoir envie de les négliger dans le cadre d'un\n",
    "  hackathon...\n",
    "\n",
    "\n",
    "* Pour un _scraping_ visant à soutenir une production statistique, il faut contacter le site pour lui laisser la\n",
    "  possibilité de s'y opposer. Indiquer son identité est aussi une bonne chose :<br/>\n",
    "  `headers = {'user-agent': \"Coucou, c'est l'Insee.\"}`<br/>\n",
    "  `response = requests.get(url, headers=headers)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comment optimiser le coût temporel d'un _scraping_ ?\n",
    "\n",
    "Le _scraping_ de nombreuses pages prend du temps, surtout si on respecte un délai entre les requêtes en accord\n",
    "avec le `robots.txt` du site visé. Le problème est dit I/O-blocant, c'est-à-dire que le coût provient surtout\n",
    "du délai nécessaire à l'envoi d'une requête HTTP et à la réception de la réponse ; parser du html est généralement\n",
    "assez rapide. On a cependant envie d'envoyer des requêtes _aussi vite que possible / permis_, plutôt que de \"perdre\"\n",
    "le petit peu de temps que prend l'extraction d'information. Une solution peut alors être de distribuer les opérations.\n",
    "\n",
    "Ci-dessous, un exemple de structure permettant de répondre à ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def download(url, headers=None):\n",
    "    \"\"\"Requête une url donnée et retourne la réponse.\"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "def parse(response):\n",
    "    \"\"\"Extrait l'information voulue d'une page html.\"\"\"\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    infos = {}\n",
    "    # Extraction d'information ici, stockées dans `infos`.\n",
    "    return infos\n",
    "\n",
    "def process(url, headers, queue):\n",
    "    response = download(url, headers)\n",
    "    results = parse(response)\n",
    "    if results is not None:\n",
    "        queue.put(results)\n",
    "\n",
    "def main(urls_list, delay=1, headers=None, pool_size=2):\n",
    "    \"\"\"Télécharge et parse une liste d'urls, en respectant un délai donné entre requêtes.\"\"\"\n",
    "    queue = multiprocessing.Manager().Queue()\n",
    "    with multiprocessing.Pool(pool_size) as pool:\n",
    "        # Toutes les <delay> secondes, on lance le traitement d'une nouvelle url.\n",
    "        for url in urls_list:\n",
    "            pool.apply_aync(func=process, args=(url, headers, queue))  # apply_async effectue la tâche en arrière-plan.\n",
    "            time.sleep(delay)\n",
    "            # Ici on s'assure de ne pas créer de file d'attente, qui pourrait entraîner le franchissement du délai.\n",
    "            while len(pool._cache) >= pool_size:\n",
    "                time.sleep(.1)\n",
    "        # Une fois toutes les url soumises, on attend que toutes les opérations de traitement soient finies.\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    # Une fois toutes les urls traitées, on récupère les résultats :\n",
    "    results = []\n",
    "    while queue.qsize() > 0:\n",
    "        results.append(queue.get())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes :\n",
    "* A la solution proposée ci-dessus, on peut préférer un système qui lance les requêtes une par une et s'assure\n",
    "  simplement que le temps d'attente est bien respecté (si la connexion est rapide ou si le délai est \"long\",\n",
    "  on ne peut pas optimiser au-delà).\n",
    "  \n",
    "  \n",
    "* Si on veut travailler sur une liste d'urls générée dynamiquement lors de l'analyse des pages initialement visées,\n",
    "  on peut ajouter un autre système de queue : le parseur y place (`queue.put`) les urls trouvées, tandis que le\n",
    "  gestionnaire de téléchargements en tire (`queue.get`) les résultats. Il suffit de définir un \"signal d'arrêt\"\n",
    "  (e.g. `None`), i.e. un élément qui, lorsqu'il est reçu, signifie que l'on a atteint le \"bout\" de la queue. Sinon,\n",
    "  on peut mettre un \"simple\" _timeout_, i.e. temps d'expiration de l'instruction d'attente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comment collecter autre chose que du html ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pour toute ressource au format textuel (fichier css, csv, txt, json...), on peut la requêter directement\n",
    "  et l'écrire dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GeoJson \"actions contre l'isolement des personnes âgées\".\n",
    "url = 'https://geo.data.gouv.fr/api/geogw/services/5779810963f06a3a8e81541b/feature-types/C1619/download?format=GeoJSON&projection=WGS84'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('actions_isolement_personnes_agees.json', 'w') as json_file:  # mode 'w' pour 'write'\n",
    "    json_file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pour d'autres formats (pdf, xls...), on requête également directement, mais plutôt que d'écrire le texte\n",
    "  déjà décodé, on stocke la version encodée du contenu téléchargé (et le lecteur de document déchiffrera).\n",
    "  L'accès à l'information encodée se fait _via_ `response.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pdf \"guide de référence Naf v2\"\n",
    "url = 'https://insee.fr/fr/statistiques/fichier/2406147/guide_naf_cpf_rev_2.pdf'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(url.rsplit('/', 1)[1], mode='wb') as pdf:  # mode 'wb' pour 'write binary'\n",
    "    pdf.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cette dernière solution marche aussi pour les images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logo de l'Insee\n",
    "url = 'https://insee.fr/static/img/logo_com_externe_semi_bold.png'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('logo.png', 'wb') as png:\n",
    "    png.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualisons le logo précédemment stocké.\n",
    "from IPython.display import Image\n",
    "Image('logo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comment gérer les cas où JavaScript nous met des bâtons dans les roues ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[En construction]\n",
    "\n",
    "* Explication : boutons JavaScript, requêtes asynchrones (Ajax).\n",
    "* Dans certains cas : la requête touche à une ressource externe (e.g. un json),\n",
    "  dont on peut récupérer l'adresse pour la requêter.\n",
    "* Dans d'autres cas : on peut faire des requêtes POST pour déclencher un évènement, s'authentifier, etc.\n",
    "* Sinon : on va émuler un navigateur web, avec Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Outil supplémentaire\n",
    "\n",
    "* `scrapy` : librairie tierce permettant de mettre en place des crawlers / scrapers.\n",
    "  * fonctionalités déjà existantes.\n",
    "  * syntaxe spécifique à cette librairie pour parser le html (mais possibilité de lui substituer BeautifulSoup)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

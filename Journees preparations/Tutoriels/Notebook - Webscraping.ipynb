{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Exercices pratiques de webscraping</h2></center>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Installation des librairies tierces utilisées.\n",
    "import pip\n",
    "for pkg in ['requests', 'bs4']:\n",
    "    pip.main(['install', pkg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 - Suivi de la \"Une\" du site Le Monde\n",
    "\n",
    "L'objectif est ici de récupérer la liste des articles en \"Une\" du site http://www.lemonde.fr/ de manière automatisée.\n",
    "\n",
    "_Note : le fichier `robots.txt` du site Le Monde interdit la navigation automatisée, aussi veillera-t-on à limiter\n",
    " au maximum le nombre de requêtes effectuées, dans cet exercice comme dans les suivants. Pour ce faire, mieux vaut\n",
    " séparer les cellules vouées au téléchargement des pages à analyser de celles où l'extraction d'information à lieu,\n",
    " puisque cette dernière est par nature soumise à un processus d'essais/erreurs, sans qu'il ne soit nécessaire de\n",
    " télécharger la page à chaque itération._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.0 Récupérer la page d'accueil du site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('http://www.lemonde.fr/')\n",
    "if response.status_code != 200:\n",
    "    print('Échec de la requête: statut %s.' % response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Récupérer l'article en \"Une\".\n",
    "\n",
    "Le premier objectif est de récupérer l'article en \"Une\" du journal _Le Monde_. Il s'agit d'extraire son titre, la ou les phrases d'accroche associées et l'url de la page de l'article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correction** :\n",
    "\n",
    "* L'étude de la page permet de repérer que l'article en Une se trouve dans une balise `<article>` de classe spécifique `titre_une`. Ces deux critères permettent alors de récupérer cet article à l'aide de `BeautifulSoup`.\n",
    "\n",
    "* Au sein de la balise extraite se trouvent des sous-balises structurant les informations ciblées : le titre est dans une balise `<h1>`, le texte dans une balise `<p>` et l'url (relative) dans la propriété `href` d'une balise `<a>`.\n",
    "\n",
    "* Optionnellement, on peut nettoyer le texte extrait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "article_une = soup.find('article', {'class': 'titre_une'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(string):\n",
    "    \"\"\"Fonction de nettoyage textuel sommaire.\"\"\"\n",
    "    # Remplace les tabluations, sauts de lignes, etc. par des espaces.\n",
    "    string = re.sub('\\n|\\r|\\t|\\xa0', ' ', string)\n",
    "    # Retire les ' .' (séparateurs utilisés dans `soup.get_text`) inappropriés.\n",
    "    string = re.sub('^\\.', '', string.replace(' .', ''))\n",
    "    # Retire les espaces inappropriés.\n",
    "    return re.sub('  +', ' ', string).strip(' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{\n",
    "    'titre': clean_text(article_une.find('h1').text),\n",
    "    'texte': clean_text(article_une.find('p').text),\n",
    "    'url': 'https://www.lemonde.fr' + article_une.find('a')['href']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Récupérer les articles en tête de section\n",
    "\n",
    "Outre la Une, certains articles sont mis en avant sur la page d'accueil du journal _Le Monde_ par l'inclusion d'un petit texte d'accroche. Le second objectif consiste à repérer ces articles et à extraire les mêmes informations que pour la Une (titre, url et descriptif).\n",
    "\n",
    "Attention, certaines urls ne sont pas faciles à identifier, du fait d'une technologie particulière permettant leur chargement lors du rendu de la page. Dans un premier temps, chercher à ne récupérer les urls que lorsqu'elles sont \"faciles\" à identifier. Optionnellement, chercher une astuce pour les retrouver (une proposition se trouve dans le corrigé de ce notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correction** :\n",
    "\n",
    "* Les articles ciblés font, comme la Une, l'objet d'une structuration au sein d'une balise `<article>` de classe spécifique `img_tt_chapo`.\n",
    "\n",
    "\n",
    "* Les informations que l'on souhaite extraire sont structurées selon des sous-balises similaires au cas de la _Une_, si ce n'est que le titre est dans une balise `<h2>` et non `<h1>`.\n",
    "\n",
    "\n",
    "* Dans certains cas cependant, l'url n'est pas directement accessible, car elle est chargée dynamiquement par un script à partir de l'attribut `data-href` d'une balise `<span>`. L'astuce proposée est alors de chercher parmi l'ensemble des urls disponibles sur la page s'il en est une qui contient les premiers mots du titre de l'article. Cette solution s'appuie sur la structure particulière des urls des articles sur le site du _Monde_, et n'est ni généralisable en toutes circonstances, ni une solution optimale ; sans doute peut-on faire mieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Version \"simple\" de la solution, avec urls manquantes.\n",
    "[\n",
    "    {\n",
    "        'titre': clean_text(article.find('h2').text),\n",
    "        'texte': clean_text(article.find('p').text),\n",
    "        'url': (\n",
    "            ('https://www.lemonde.fr' + article.find('a')['href'])\n",
    "            if article.find('a') is not None else ''\n",
    "        )\n",
    "    }\n",
    "    for article in soup.find_all('article', {'class': 'img_tt_chapo'})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonctions auxiliaires pour trouver certaines des urls manquantes.\n",
    "\n",
    "def get_href_from_title(titre, soup):\n",
    "    \"\"\"Cherche un lien parmi ceux d'une page concordant avec un titre donné.\"\"\"\n",
    "    # Nettoyage du titre.\n",
    "    titre = retirer_accents(titre.lower())\n",
    "    titre = re.sub('^ | $', '', re.sub('  +', ' ', re.sub('[^a-z ]', '', titre)))\n",
    "    # Sous-sélection et formattage des trois premiers termes du titre.\n",
    "    title_part = '-'.join(titre.split(' ')[:3])\n",
    "    # Itération sur les urls liés dans la page, jusqu'à en trouver une qui concorde.\n",
    "    for anchor in soup.find_all('a'):\n",
    "        if title_part in anchor.get('href', ''):\n",
    "            return 'https://www.lemonde.fr' + anchor['href']\n",
    "\n",
    "        \n",
    "def retirer_accents(text):\n",
    "    \"\"\"Remplace les caractères accentués par leur équivalent sans accent.\"\"\"\n",
    "    charmap = [\n",
    "            ('àâ', 'a'), ('ç', 'c'), ('éèêë', 'e'),\n",
    "            ('îï', 'i'), ('ôö', 'o'), ('ùûü', 'u')\n",
    "        ]\n",
    "    for character in charmap:\n",
    "        text = re.sub('[%s]' % character[0], character[1], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Version \"complexe\" utilisant les fonctions précédentes.\n",
    "\n",
    "def parse_head_article(article, soup):\n",
    "    \"\"\"Extrait le titre, le texte et l'url associés à un article \"mis en avant\".\"\"\"\n",
    "    parsed = {\n",
    "        'titre': clean_text(article.find('h2').text),\n",
    "        'texte': clean_text(article.find('p').text)\n",
    "    }\n",
    "    parsed['url'] = (\n",
    "        get_href_from_title(parsed['titre'], soup)\n",
    "        if article.find('a') is None\n",
    "        else 'https://www.lemonde.fr' + article.find('a')['href']\n",
    "    )\n",
    "    return parsed\n",
    "\n",
    "articles = [\n",
    "    parse_head_article(article, soup)\n",
    "    for article in soup.find_all('article', {'class': 'img_tt_chapo'})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Affichage \"lisible\" des résultats.\n",
    "\n",
    "def format_article(article_dict):\n",
    "    return '%s\\n\\n%s\\n\\n%s' % (article_dict['titre'], article_dict['texte'], article_dict['url'])\n",
    "\n",
    "\n",
    "print('\\n\\n\\n'.join(map(format_article, articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Récupérer les titres et adresses de tous les articles listés sur la page d'accueil\n",
    "\n",
    "La page d'accueil du journal _Le Monde_ comporte des liens vers de nombreux articles, différemment mis en avant mais se composant _a minima_ d'un titre et d'un lien pointant vers l'url de la page de l'article. Le dernier objectif consiste à repérer ces articles dans le html récupéré, et à extraire pour chacun son titre et son url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correction** :\n",
    "\n",
    "* Tous les articles sont rangés dans la section \"articles\" du site, une solution simple est donc d'extraire l'ensemble des urls présents dans le html (attributs `href` des balises `<a>`), et de filtrer celles répondant à ce schéma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_lien(tag):\n",
    "    \"\"\"Parse un lien d'article sur le site du Monde.\"\"\"\n",
    "    url = tag['href']\n",
    "    if tag.text != '':\n",
    "        text = tag.text\n",
    "    elif tag.find('h1') is not None:\n",
    "        text = tag.find('h1').text\n",
    "    elif tag.find('h2') is not None:\n",
    "        text = tag.find('h2').text\n",
    "    else:\n",
    "        text = ''\n",
    "    return {re.sub('^\\d\\d:\\d\\d', '', clean_text(text)): url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liens = [parse_lien(a) for a in soup.find_all('a') if '/article/' in a['href']]\n",
    "\n",
    "print('\\n\\n'.join(map(str, liens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 - Récupérer les résultats d'une recherche Google (ou Bing, ou DuckDuckGo...)\n",
    "\n",
    "Si la légalité de la pratique est encore une fois à vérifier (le _robots.txt_ de Google l'interdit, par exemple - au profit d'un module spécifique limitant drastiquement le nombre de requêtes, sauf à payer pour celles-ci), il peut être intéressant d'effectuer une recherche _via_ un moteur de recherche et de récupérer automatiquement les urls listées. Ceci peut en particulier permettre de contourner l'absence de moteur de recherche sur un site (ou la difficulté à manipuler celui-ci sans recourir à un navigateur).\n",
    "\n",
    "**Note** : Google est hégémonique (pour le meilleur et pour le pire), et est le premier mentionné dans la suite de l'exercice. Bing (lui aussi produit d'une grosse compagnie) et DuckDuckGo (qui veut du bien à votre vie privée) sont également traités dans le corrigé, et chacun devrait se sentir libre d'utiliser l'un de ces moteurs de recherche, ou tout autre service similaire de son choix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Effectuer une recherche Google (ou autre) avec Requests\n",
    "\n",
    "La première étape (comme toujours) est de récupérer le html sur lequel on veut travailler - en l'occurence celui associé à une recherche donnée. En amont, il faut donc définir l'url correspondant à ladite requête. L'exercice consiste donc à rechercher le terme \"insee\" sur Google, Bing ou DuckDuckGo, et à récupérer le html associé à cette recherche.\n",
    "\n",
    "_Conseil_ : dans le cas de Google, afin de \"simplifier\" l'url, utiliser https://encrypted.google.com/.\n",
    "\n",
    "Certains paramètres (pour changer de page, régionaliser les résultats, etc.) ne sont pas toujours évidents à identifier, car ils peuvent être cachés. Lire les scripts présents dans le html peut aider, faire des tests dans un navigateur aussi. Le corrigé présente quelques options utiles identifiées pour les trois moteurs de recherche étudiés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correction** :\n",
    "\n",
    "Afin de trouver la structure de l'url à utiliser, le plus simple est d'effectuer une recherche quelconque sur le moteur de recherche, et de \"déconstruire\" celle-ci. Elle est toujours de la forme `<protocole>://<nom de domaine><éventuel raffinement>?<bloc d'options>`, où le dernier bloc est de la forme `<option>=<valeur>` avec un séparateur `&` entre chaque couple option / valeur.\n",
    "\n",
    "* Google :\n",
    "  * protocole, domaine et extension : https://encrypted.google.com/search\n",
    "  * recherche : q=insee (pour une recherche sur le terme \"insee\")\n",
    "  * page : \n",
    "  * régionalisation de la recherche : hl=fr (pour la France)\n",
    "  * _exemple_ : https://encrypted.google.com/search?hl=fr&q=insee\n",
    "\n",
    "\n",
    "* Bing :\n",
    "  * protocole, domaine et extension : https://www.bing.com/search\n",
    "  * recherche : q=insee (pour une recherche sur le terme \"insee\")\n",
    "  * page: first=9 (pour obtenir dix résultats (dont publicités), à partir du neuvième (hors publicités))\n",
    "  * _exemple_ : https://www.bing.com/search?q=insee\n",
    "\n",
    "\n",
    "* DuckDuckGo : \n",
    "  * protocole, domaine et extension : https://duckduckgo.com/html\n",
    "      * Note : dans le navigateur, la section html n'est pas utilisée. Elle est requêtée de manière sous-jacente de\n",
    "        manière asynchrone _via_ un script JavaScript. En spécifiant l'adresse directement, on contourne cette\n",
    "        spécificité, afin de récupérer \"en clair\" les résultats de la recherche.\n",
    "  * recherche : q=insee\n",
    "  * régionalisation de la recherche : kl=fr (pour la France)\n",
    "  * page : s=30\n",
    "    (pour obtenir trente (déterministe) résultats, à partir du trente-et-unième (spécifié) ; équivalent de la page 2)\n",
    "  * spécifier un site auquel restreindre les résultats : site%3Awww.societe.com (pour une recherche sur www.societe.com),\n",
    "    à inclure dans la recherche\n",
    "  * _exemple_ : https://duckduckgo.com/html/?q=insee+site%3Awww.societe.com&kl=fr\n",
    "\n",
    "\n",
    "**Note** : pour rechercher plusieurs termes à la fois (dans les trois cas précédents), séparer ces termes d'un `+` dans la valeur attribuée à l'option de recherche `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response_google = requests.get(\"https://encrypted.google.com/search?hl=fr&q=insee\")\n",
    "if response_google.status_code != 200:\n",
    "    print('Erreur avec Google: %s.' % response_google.status_code)\n",
    "\n",
    "response_bing = requests.get(\"https://www.bing.com/search?q=insee\")\n",
    "if response_bing.status_code != 200:\n",
    "    print('Erreur avec Bing: %s.' % response_bing.status_code)\n",
    "\n",
    "response_duckduckgo = requests.get(\"https://duckduckgo.com/html/?q=insee&kl=fr\")\n",
    "if response_duckduckgo.status_code != 200:\n",
    "    print('Erreur avec DuckDuckGo: %s.' % response_duckduckgo.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Extraire les résultats de la recherche\n",
    "\n",
    "L'objectif est maintenant d'identifier les urls listées sur les pages de résultats précédemment requêtées. La solution diffère pour chaque moteur de recherche, et s'appuie sur les procédés de lecture de html et d'essais/erreurs déjà mobilisés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correction** :\n",
    "\n",
    "Dans chaque cas, les spécificités d'extraction des résultats diffèrent. Ci-dessous, une solution possible pour chacun des trois moteurs de recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup_google = BeautifulSoup(response_google.text, 'html.parser')\n",
    "soup_bing = BeautifulSoup(response_bing.text, 'html.parser')\n",
    "soup_duckduckgo = BeautifulSoup(response_duckduckgo.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liens_google = [\n",
    "    a['href'][7:]\n",
    "    for a in soup_google.find_all('a')\n",
    "    if a.get('href', '').startswith('/url')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liens_bing = [\n",
    "    li.find('a')['href']\n",
    "    for li in soup_bing.find_all('li', {'class': 'b_algo'})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liens_duckduckgo = [\n",
    "    a['href'][15:].replace('%3A', ':').replace('%2F', '/')\n",
    "    for a in soup_duckduckgo.find_all('a', {'class': 'result__url'})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Voilà pour ces exercices introductifs ; bon courage pour la suite et la mise en pratique !**</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
